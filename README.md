# ğŸ¬ Sentiment Analysis on IMDB Movie Reviews 

This project performs **binary sentiment classification** (positive or negative) on IMDB movie reviews using **LSTM-based deep learning models**. It includes data preprocessing, tokenization, model building, evaluation, and hyperparameter tuning â€” all modularized into 13 cleanly organized Python scripts under the `code/` folder.

---

## ğŸ“Œ Table of Contents

- [ğŸ“– Project Overview](#-project-overview)
- [ğŸ“Š Dataset](#-dataset)
- [ğŸ§° Tech Stack](#-tech-stack)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ“ˆ Results](#-results)
- [ğŸš€ Future Improvements](#-future-improvements)

---

## ğŸ“– Project Overview

This sentiment analysis project uses an LSTM model to classify movie reviews from the IMDB dataset. The model is trained on cleaned and preprocessed text data, using techniques such as stopword removal, tokenization, padding, and dropout regularization. The project includes both a baseline LSTM model and a tuned version with a learning rate scheduler.

---

## ğŸ“Š Dataset

- **Name**: IMDB Movie Reviews Dataset
- **Size**: 50,000 reviews (25k positive, 25k negative)
- **Source**: [Kaggle - IMDB Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
- **Format**: CSV with two columns: `review` and `sentiment`

âš ï¸ **Note**: The dataset is not uploaded due to redistribution restrictions. Please download it from the Kaggle link and place it in the root directory.

---

## ğŸ§° Tech Stack

### Languages & Libraries
- Python 3.x
- TensorFlow / Keras
- NumPy / Pandas
- Scikit-learn
- NLTK
- Matplotlib / Seaborn

### ML Techniques
- LSTM Neural Network
- Dropout Regularization
- Learning Rate Scheduler (`ReduceLROnPlateau`)

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ 01_setup_and_imports.py
â”‚   â”œâ”€â”€ 02_load_and_preview_data.py
â”‚   â”œâ”€â”€ 03_data_exploration_and_summary.py
â”‚   â”œâ”€â”€ 04_visualize_sentiment_distribution.py
â”‚   â”œâ”€â”€ 05_text_cleaning_and_preprocessing.py
â”‚   â”œâ”€â”€ 06_tokenization_and_encoding.py
â”‚   â”œâ”€â”€ 07_train_test_split.py
â”‚   â”œâ”€â”€ 08_model_definition_and_compilation.py
â”‚   â”œâ”€â”€ 09_model_training.py
â”‚   â”œâ”€â”€ 10_model_evaluation_and_metrics.py
â”‚   â”œâ”€â”€ 11_plot_training_history.py
â”‚   â”œâ”€â”€ 12_model_tuning_and_redefinition.py
â”‚   â””â”€â”€ 13_training_with_lr_scheduler.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ accuracy_plot.png
â”‚   â”œâ”€â”€ loss_plot.png
â”‚   â””â”€â”€ sentiment_distribution.png
â”œâ”€â”€ models/
â”‚   â””â”€â”€ sentiment_model.h5
â”œâ”€â”€ IMDB Dataset.csv     # Not uploaded â€” download manually
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


##  ğŸ“ˆ Results
| Metric    | Value |
| --------- | ----- |
| Accuracy  | \~85% |
| Precision | \~86% |
| Recall    | \~84% |
| F1 Score  | \~85% |


## ğŸš€ Future Improvements

âœ… Add Attention Mechanism
âœ… Integrate GloVe/Word2Vec embeddings
âœ… Try transformer-based models (like BERT)
âœ… Deploy model using Flask or FastAPI
âœ… Add model explainability tools (LIME, SHAP)

